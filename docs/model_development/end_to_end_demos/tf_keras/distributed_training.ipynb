{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Training\n",
    "\n",
    "Deep learning models are infamous in their resources needs and not having these resources available can make Large models and / or large data slow the training time drastically, making it almost impossible to run at a reasonable time. Luckly for us, MLRun can assign GPUs and distribute the training with ease, all from the same `apply_mlrun` function we saw in the previous notebook.\n",
    "\n",
    "In this notebook, we will train a large MNIST model by assigning GPUs and distribute the training using Horovod.\n",
    "\n",
    "1. [Open MPI and Horovod](#section_1)\n",
    "2. [Load the Projet](#section_2)\n",
    "3. [Run Distributed Training](#section_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "<a id=\"section_1\"></a>\n",
    "\n",
    "## 1. Open MPI and Horovod\n",
    "\n",
    "MLRun is using Horovod over OpenMPI to run distributed training, we will have a brief explanation on both before we continue with loading the project and running our training function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Open MPI\n",
    "\n",
    "<img src=\"https://www.open-mpi.org/images/open-mpi-logo.png\" alt=\"Open MPI logo\" width=\"150\"/>\n",
    "\n",
    "From the official [**Open MPI**](https://www.open-mpi.org/) website: *The Open MPI Project is an open source Message Passing Interface implementation that is developed and maintained by a consortium of academic, research, and industry partners. Open MPI is therefore able to combine the expertise, technologies, and resources from all across the High Performance Computing community in order to build the best MPI library available. Open MPI offers advantages for system and software vendors, application developers and computer science researchers.*\n",
    "\n",
    "To use Open MPI in MLRun, when creating a MLRun function all that is needed to be done is to set the `kind` parameter to `\"mpijob\"`. We will cover it in chapter 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Horovod\n",
    "\n",
    "<img src=\"https://horovod.readthedocs.io/en/stable/_static/logo.png\" alt=\"Horovod logo\" width=\"150\"/>\n",
    "\n",
    "From the official [**Horovod**](https://horovod.ai/) website: *Horovod was originally developed by Uber to make distributed deep learning fast and easy to use, bringing model training time down from days and weeks to hours and minutes.*\n",
    "\n",
    "Becuase we used `apply_mlrun` in our code, MLRun will take care of all the configuration required to distribute the training. All we need to do is to create function with `kind=\"mpijob\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "<a id=\"section_2\"></a>\n",
    "\n",
    "## 2. Load the Project\n",
    "\n",
    "We will load our project using the same function we used to create it: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlrun\n",
    "\n",
    "# Set our project's name:\n",
    "project_name = \"mnist-classifier\"\n",
    "\n",
    "# Create the project:\n",
    "project = mlrun.get_or_create_project(name=project_name, context=\"./\", user_project=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "<a id=\"section_3\"></a>\n",
    "\n",
    "## 3. Run Distributed Training\n",
    "\n",
    "Although it may sound complicated, in MLRun a distributed training job is just another job. We will need to create an Open MPI job from our training code, configure the amount of workers and run it. That's all!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Create the Funnction\n",
    "\n",
    "We will use `code_to_function` again on the model development notebook, but this time we will set `kind` to `\"mpijob\"`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the function parsing the model development code from previous notebook using 'code_to_function':\n",
    "mpijob_development_function = mlrun.code_to_function(\n",
    "    filename=\"./model_development.ipynb\",  # <- We set the function to get the code from this notebook.\n",
    "    name=\"mnist-mpijob-development\",\n",
    "    kind=\"mpijob\",  # <- Notice we set kind to be an OpenMPI job.\n",
    ")\n",
    "\n",
    "# Save the function in the project:\n",
    "project.set_function(mpijob_development_function)\n",
    "project.save()\n",
    "\n",
    "# Mount it:\n",
    "mpijob_development_function.apply(mlrun.platforms.auto_mount())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Configure the Function\n",
    "\n",
    "We will configure to train with 2 workers. In addition, we will choose what resources to use for each worker, set `use_gpu` to `True` if you wish to use GPUs, the image will be set accorindgly (hence we didn't set it at `code_to_function` using the `image` attribute). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select whether to use GPUs:\n",
    "use_gpu = True\n",
    "\n",
    "# Set the image and GPUs if needed:\n",
    "if use_gpu:\n",
    "    mpijob_development_function.spec.image = \"mlrun/ml-models-gpu\"\n",
    "    # Select the number of GPUs per replica:\n",
    "    mpijob_development_function.gpus(1)\n",
    "else:\n",
    "    mpijob_development_function.spec.image = \"mlrun/ml-models\"\n",
    "\n",
    "# Setup the number of workers for training:\n",
    "mpi_training_and_evaluation_function.spec.replicas = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Run a Distributed Training MPIJob\n",
    "\n",
    "Let's call `run` and see our training in action. Notice the number of steps is half as it was, as now each worker is using half of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the function in MLRun's UI, we can see in the `pods` tab the workers we assign (2 in our case). If chosen to use GPUs and your system has GPUs scaled to zero, it may take time to assign them, so the `pods` tab is a great visual way to see the state of your workers:\n",
    "\n",
    "<img src=\"./mlrun_ui_results.png\" alt=\"MLRun UI pods tab screenshot\" width=\"1200\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the distributed training job:\n",
    "distributed_training_run = mpi_training_and_evaluation_function.run(\n",
    "    name=\"distributed-training\",\n",
    "    params={\n",
    "        \"conv_blocks\": 4,\n",
    "        \"dense_blocks\": 4,\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"batch_size\": 128,\n",
    "        \"epochs\": 20,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "Lastly, the [**next chapter**](./model_serving.ipynb) will be around serving our model in a realtime serverless function with a cool Jupyter application, see you there!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
