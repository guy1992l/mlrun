{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Serving\n",
    "\n",
    "We now wish to serve our model using **nuclio** in order to send an image of a digit we will write and classify it into its equivalnt numeric value. \n",
    "\n",
    "1. [Code Review](#section_1)\n",
    "2. [Serving Graph](#section_2)\n",
    "3. [Test](#section_3)\n",
    "4. [Deploy](#section_4)\n",
    "\n",
    "<img src=\"./mnist_serving_graph.png\" alt=\"MNIST serving graph\" width=\"1200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "<a id=\"section_1\"></a>\n",
    "\n",
    "## 1. Code Review\n",
    "\n",
    "The serving code will have the following components as seen in the image above:\n",
    "\n",
    "1. Import NumPy and PIL to handle the images.\n",
    "2. Preprocess the inputs.\n",
    "3. Postprocess the model outputs.\n",
    "\n",
    "We will review each component briefly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Module Imports\n",
    "\n",
    "We will use PIL to grayscale and resize the input image and NumPy to hold the image as an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlrun: start-code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Union\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Preprocess the Inputs\n",
    "\n",
    "Before inferring the images thorugh the model, each image must be in shape of 28x28x1. So, we will grayscale the image to have only 1 channel and resize it to 28x28."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(event: dict) -> Dict[str, List[np.ndarray]]:\n",
    "    # Read the image:\n",
    "    digit_image = Image.fromarray(np.array(event[\"inputs\"]))\n",
    "    \n",
    "    # Reduce the channels from 4 (RGBA) to 1 (Grayscale):\n",
    "    digit_image = ImageOps.grayscale(digit_image)\n",
    "\n",
    "    # Resize the image:\n",
    "    digit_image = digit_image.resize((28, 28))\n",
    "\n",
    "    # Convert to numpy array:\n",
    "    digit_image = np.expand_dims(np.array(digit_image), 0)\n",
    "\n",
    "    # Pack and return:\n",
    "    return {\"inputs\": [digit_image]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Postprocess the model Outputs\n",
    "\n",
    "Process the Softmax layer output of the model into a dictionary holding the digit prediction and confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(event: dict) -> Dict[str, Union[int, float]]:\n",
    "    # Read the prediction from the model:\n",
    "    prediction = np.squeeze(event[\"outputs\"])\n",
    "    \n",
    "    # Get the digit:\n",
    "    predicted_digit = prediction.argmax()\n",
    "    \n",
    "    # Get the confidence:\n",
    "    predicted_confidence = prediction[predicted_digit]\n",
    "    \n",
    "    # Parse and return:\n",
    "    return {\n",
    "        \"digit\": predicted_digit,\n",
    "        \"confidence\": predicted_confidence,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlrun: end-code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "<a id=\"section_2\"></a>\n",
    "\n",
    "## 2. Serving Graph\n",
    "\n",
    "We will build the serving graph using the serving code in this notebook. To do so, we first need a project to get the model artifact from and then create our serving function. \n",
    "\n",
    "In addition to the code in this notebook, the model itself will be handled by a model server. We will use the basic TensorFlow.Keras model server available in MLRun at `mlrun.frameworks.tf_keras.TFKerasModelServer`. \n",
    "\n",
    "> You can inherit it or create your own Model Server class `ModelServer`. To learn more check out the docs of [serving]()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Get the Project\n",
    "\n",
    "We will use the `get_or_create_project` function to get our project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlrun\n",
    "\n",
    "# Set our project's name:\n",
    "project_name = \"mnist-classifier\"\n",
    "\n",
    "# Create the project:\n",
    "project = mlrun.get_or_create_project(name=project_name, context=\"./\", user_project=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Create a MLRun Function\n",
    "\n",
    "We will use the `code_to_function` function as seen before to convert the code in this notebook into a MLRun Function. This time we will set the function's kind as `\"serving\"`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the function parsing the notebook's code using 'code_to_function':\n",
    "serving_function = mlrun.code_to_function(\n",
    "    name=\"mnist-serving\",\n",
    "    kind=\"serving\",  # <- Serving function kind.\n",
    "    image=\"mlrun/ml-models\"\n",
    ")\n",
    "\n",
    "# Save the function in the project:\n",
    "project.set_function(serving_function)\n",
    "project.save()\n",
    "\n",
    "# Mount it:\n",
    "serving_function.apply(mlrun.platforms.auto_mount())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Get the Model\n",
    "\n",
    "We will get our model using `get_artifact_uri`. Feel free to use different tags and models you trained yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the model to load:\n",
    "model_name = \"mnist_model\"\n",
    "\n",
    "# Get the model artifact uri:\n",
    "model_path = project.get_artifact_uri(key=model_name, category=\"model\")\n",
    "\n",
    "# Choose the ModelServer according to the selected framework:\n",
    "model_server_class = \"mlrun.frameworks.tf_keras.TFKerasModelServer\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Build the Serving Graph\n",
    "\n",
    "Our serving graph will have the following structure: Preprocess &rarr; Model Inference &rarr; Postprocess\n",
    "\n",
    "We will use the `set_topology` method to initialize and get the graph object. From there we will use the graph's `to` method to build its steps, sending the inputs to **preprocess**, then to the **Model Server** and finally to **postprocess** where we will call to `respond`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the topology and get the graph object:\n",
    "graph = serving_function.set_topology(\"flow\", engine=\"async\")\n",
    "\n",
    "# Build the serving graph:\n",
    "graph.to(handler=\"preprocess\", name=\"preprocess\")\\\n",
    "     .to(class_name=model_server_class, name=\"mnist_classifier\", model_path=model_path)\\\n",
    "     .to(handler=\"postprocess\", name=\"postprocess\").respond()\n",
    "\n",
    "# Plot to graph:\n",
    "graph.plot(rankdir='LR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "<a id=\"section_3\"></a>\n",
    "\n",
    "## 3. Test \n",
    "\n",
    "To test our serving function, we will create a mock server (simulator). Its like deploying the serving graph locally and this way we can debug more easily and quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Create a Mock Server\n",
    "\n",
    "We will create a mock server (simulator) using the function `to_mock_server()` and test the graph with the following image:\n",
    "\n",
    "![](./mnist_digit.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our server:\n",
    "server = serving_function.to_mock_server()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Run the Test\n",
    "\n",
    "In order to send the image to the mock server, we will use the `server.test` method. This method expects two main arguments:\n",
    "* `path` - Path to the method inside the `TFKerasModelServer`. We will be using the `predict` method.\n",
    "* `body` - A JSON serializable input to send to our serving pipeline.\n",
    "\n",
    "We will load our image and send it, expecting to get **5** with high confidence!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Load our test image:\n",
    "test_image = np.array(Image.open(\"./mnist_digit.png\"))\n",
    "\n",
    "# Infer thourgh the serving graph:\n",
    "server.test(path='/predict', body={\"inputs\": test_image})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "<a id=\"section_4\"></a>\n",
    "\n",
    "## 4. Deploy\n",
    "\n",
    "Now we will deploy a realtime serverless function. If you installed `ipycanvas` you can draw your own numbers and send them to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Deploy a Realtime Serverless Function\n",
    "\n",
    "Deploying is very easy, simply call the `deploy` method to deploy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy:\n",
    "serving_function.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Draw and Classify\n",
    "\n",
    "In order to test our realtime serving function, we can use the method `invoke` which takes the same two argumetns as `test` of the mock server.\n",
    "\n",
    "With `ipycanvas` we will create a canvas for drawing digits and send them to the model by running the code block after. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import HBox, Button\n",
    "from ipycanvas import Canvas, hold_canvas\n",
    "\n",
    "# Create the canvas:\n",
    "width = 200\n",
    "height = 200\n",
    "canvas = Canvas(width=width, height=height, sync_image_data=True)\n",
    "\n",
    "# Define the 'on_mouse' actions functions to draw on the canvas:\n",
    "is_drawing = False  # Whether to draw on the canvas.\n",
    "position = None  # type: Tuple[float, float]  # Last known position holder.\n",
    "\n",
    "def on_mouse_down(x, y):\n",
    "    global is_drawing\n",
    "    global position\n",
    "    \n",
    "    # Mark drawing should begin and store the first position:\n",
    "    is_drawing = True\n",
    "    position = (x, y)\n",
    "\n",
    "def on_mouse_move(x, y):\n",
    "    global is_drawing\n",
    "    global position\n",
    "    \n",
    "    # If mouse is not down while moving (not drawing) return:\n",
    "    if not is_drawing:\n",
    "        return\n",
    "    \n",
    "    # Draw while saving the last position:\n",
    "    with hold_canvas(canvas):\n",
    "        canvas.stroke_line(position[0], position[1], x, y)\n",
    "        position = (x, y)\n",
    "\n",
    "def on_mouse_up(x, y):\n",
    "    global is_drawing\n",
    "    global position\n",
    "\n",
    "    # Mark drawing has ended:\n",
    "    is_drawing = False\n",
    "    \n",
    "    # Draw the last dot:\n",
    "    canvas.stroke_line(position[0], position[1], x, y)\n",
    "\n",
    "# Set the 'on_mouse' actions:\n",
    "canvas.on_mouse_down(on_mouse_down)\n",
    "canvas.on_mouse_move(on_mouse_move)\n",
    "canvas.on_mouse_up(on_mouse_up)\n",
    "\n",
    "# Configure the canvas style:\n",
    "canvas.fill_style = \"black\"  # MNIST are with black background.\n",
    "canvas.stroke_style = \"white\"  # MNIST digits are white.\n",
    "canvas.line_width = 15\n",
    "canvas.line_cap = 'round'\n",
    "canvas.fill_rect(0, 0, width, height)\n",
    "\n",
    "# Create the 'clear' button:\n",
    "clear_button = Button(\n",
    "    description='Clear',\n",
    "    disabled=False,\n",
    "    button_style='',\n",
    "    tooltip='Clear the canvas drawings',\n",
    "    icon='trash'\n",
    ")\n",
    "\n",
    "# Define the 'on_click' action function:\n",
    "def on_click(button):\n",
    "    canvas.fill_rect(0, 0, width, height)\n",
    "\n",
    "# Set the 'on_click' action function:\n",
    "clear_button.on_click(on_click)\n",
    "\n",
    "# Present the canvas and the button on a horizontal layout:\n",
    "HBox([canvas, clear_button])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "# Get the image drawn from the canvas:\n",
    "digit_image = canvas.get_image_data()\n",
    "\n",
    "# Infer thourgh the realtime serving graph:\n",
    "serving_function.invoke(path='/predict', body={\"inputs\": digit_image})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "**We hope you had fun MLRunning!**\n",
    "\n",
    "If you have any comments, questions, suggestions, feel free to contact us at mlrun@iguazio.com!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
